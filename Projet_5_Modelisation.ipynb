{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss, multilabel_confusion_matrix, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "import xml.etree.ElementTree as etree\n",
    "import time\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.min_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = pd.read_csv('Data/Posts_Clean_1.csv', index_col=0, converters={'Tags_list' : str, 'Text' : str})\n",
    "X = post[\"Text\"].values\n",
    "Y = post[\"Tags_list\"].str.split().values\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "  X, Y, test_size=0.2, random_state=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_binarizer = MultiLabelBinarizer(sparse_output=True)\n",
    "tags_binarizer.fit(post[\"Tags_list\"].str.split())\n",
    "y_train_binar = tags_binarizer.transform(post[\"Tags_list\"].str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervis√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_binarizer = MultiLabelBinarizer()\n",
    "tags_binarizer.fit(y_train)\n",
    "y_train_binar = tags_binarizer.transform(y_train)\n",
    "y_test_binar = tags_binarizer.transform(y_test)\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=False, token_pattern=r\"\\S+\", max_features=10000)\n",
    "count_vect.fit(x_train)\n",
    "x_train_counts = count_vect.transform(x_train)\n",
    "x_test_counts = count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
